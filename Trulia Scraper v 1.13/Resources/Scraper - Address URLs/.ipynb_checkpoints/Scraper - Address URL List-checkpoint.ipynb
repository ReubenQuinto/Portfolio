{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## This function saves a designated URL to .txt\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "\n",
    "def init_browser():\n",
    "    # Chrome driver\n",
    "    executable_path = {\"executable_path\": \"chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "# Random Sleep Variable Time\n",
    "def rand_sleep():\n",
    "    time.sleep(random.randint(15,30))\n",
    "\n",
    "# Save website to txt\n",
    "def sandbox_scrape_soup(url):\n",
    "    \n",
    "    # Initialize: Browser\n",
    "    browser = init_browser()\n",
    "    \n",
    "    browser.visit(url)\n",
    "    \n",
    "    # Create: Soup obj\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    # Create and Save: Store the soup obj into a text file (The purpose is to bypass: Google Image Verification Security)\n",
    "    # This text file will be overwritten each time this function is called\n",
    "    f = open(\"staging_soup.txt\", \"w\")\n",
    "    f.write(str(soup))\n",
    "    f.close()\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "\n",
    "sandbox_scrape_soup(\"https://www.trulia.com/property-sitemap/CA/Orange-County-06059/92603/Bethany_Dr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import: Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Create: MISC Functions\n",
    "\n",
    "def init_browser():\n",
    "    # Chrome driver\n",
    "    executable_path = {\"executable_path\": \"chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "# Random Sleep Variable Time\n",
    "def rand_sleep():\n",
    "    time.sleep(random.randint(60,240))\n",
    "    \n",
    "# Create: MAIN Functions\n",
    "   \n",
    "# Testing Function: Save website to txt > scrape the .txt file\n",
    "def scrape_addr_urls(url, idx):\n",
    "    \n",
    "    # Initialize: Browser\n",
    "    browser = init_browser()\n",
    "    \n",
    "    # Visiting: Browser\n",
    "    browser.visit(url)\n",
    "    \n",
    "    # Create: Soup obj\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        # Create and Save: Store the soup obj into a text file (The purpose is to bypass: Google Image Verification Security)\n",
    "        # This text file will be overwritten each time this function is called\n",
    "        f = open(\"staging_soup.txt\", \"w\")\n",
    "        f.write(str(soup))\n",
    "        f.close()\n",
    "    except:\n",
    "        pass\n",
    "       \n",
    "    # Requirement to bypass Google Image Verification Secuirty\n",
    "    rand_sleep()\n",
    "    \n",
    "    # Closing: Browser\n",
    "    browser.quit()\n",
    "    \n",
    "    ########## Begin Scrape: Scraping the text file (Not the URL)\n",
    "    \n",
    "    try:\n",
    "        # Create: A soup obj from the .txt file mentioned above\n",
    "        f = open(\"staging_soup.txt\")\n",
    "        soup = bs(f, \"html.parser\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Create: Target obj\n",
    "        target = soup.find_all(\"ul\", class_=\"all-properties\")[0].find_all(\"li\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    df_street_url_3 = pd.read_csv(\"Address URL List - PreProcessing (2 of 2).csv\", header=0, names=[\"zip\",\n",
    "                                                                                                    \"street\",\n",
    "                                                                                                    \"units\",\n",
    "                                                                                                    \"type\",\n",
    "                                                                                                    \"url_addr\"])\n",
    "    # Append: URL addresses\n",
    "    for y in range(len(target)):\n",
    "\n",
    "        # Initialize: Python dictionary (To store the scraped data)\n",
    "        dict_url_addr = {}\n",
    "\n",
    "        zip_cd = df_street_url_3[\"zip\"][idx],\n",
    "        street = df_street_url_3[\"street\"][idx],\n",
    "        units = df_street_url_3[\"units\"][idx],\n",
    "        house_type = df_street_url_3[\"type\"][idx],\n",
    "        url_addr = target[y].a.get(\"href\")\n",
    "\n",
    "        # Add and Clean: Scraped data to dict\n",
    "        dict_url_addr = {\n",
    "            \"zip\": \"\".join(str(zip_cd)).replace(\"(\",\"\").replace(\",\",\"\").replace(\")\",\"\"),\n",
    "            \"street\": \"\".join(str(street)).replace(\"(\",\"\").replace(\",\",\"\").replace(\")\",\"\").replace(\"'\",\"\"),\n",
    "            \"units\": \"\".join(str(units)).replace(\"(\",\"\").replace(\",\",\"\").replace(\")\",\"\"),\n",
    "            \"type\": \"\".join(str(house_type)).replace(\"(\",\"\").replace(\",\",\"\").replace(\")\",\"\").replace(\"'\",\"\"),\n",
    "            \"url_addr\": \"\".join(str(url_addr)).replace(\"(\",\"\").replace(\",\",\"\").replace(\")\",\"\")\n",
    "        }\n",
    "\n",
    "        df_street_url_2 = pd.read_csv(\"Address URL List - FINAL.csv\")\n",
    "\n",
    "        # Append: Dataframe to Database\n",
    "        df_street_url_2 = df_street_url_2.append(dict_url_addr, ignore_index=True)\n",
    "\n",
    "        # Save: Info to Dataframe\n",
    "        df_street_url_2.to_csv(\"Address URL List - FINAL.csv\", index=False, columns=[\"zip\",\n",
    "                                                                                     \"street\",\n",
    "                                                                                     \"units\",\n",
    "                                                                                     \"type\",\n",
    "                                                                                     \"url_addr\"\n",
    "                                                                                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in URL Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create: A DF to read in the \"staging_address.csv\"\n",
    "df_street_url = pd.read_csv(\"Address URL List - PreProcessing (2 of 2).csv\")\n",
    "\n",
    "# Initialize: List\n",
    "street_url_list = []\n",
    "\n",
    "# Scrape: Address + append to address_url_list = []\n",
    "for i in range(len(df_street_url)):\n",
    "    street_url_list.append(df_street_url[\"url\"][i])\n",
    "\n",
    "for i in range(len(df_street_url)):\n",
    "    print(f'Row: {str(i + 1)} | URL: {street_url_list[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Main Function via For Loop"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# This code is: FOR TESTING\n",
    "\n",
    "try:\n",
    "    # Create: A soup obj from the .txt file mentioned above\n",
    "    f = open(\"staging_soup.txt\")\n",
    "    soup = bs(f, \"html.parser\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Create: Target obj\n",
    "    target = soup.find_all(\"ul\", class_=\"all-properties\")[0].find_all(\"li\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_street_url_3 = pd.read_csv(\"Address URL List - PreProcessing (2 of 2).csv\", header=0, names=[\"zip\",\n",
    "                                                                                                \"street\",\n",
    "                                                                                                \"units\",\n",
    "                                                                                                \"type\",\n",
    "                                                                                                \"url_addr\"])\n",
    "# Append: URL addresses\n",
    "for y in range(len(target)):\n",
    "\n",
    "    # Initialize: Python dictionary (To store the scraped data)\n",
    "    dict_url_addr = {}\n",
    "\n",
    "    zip_cd = df_street_url_3[\"zip\"][1],\n",
    "    street = df_street_url_3[\"street\"][1],\n",
    "    units = df_street_url_3[\"units\"][1],\n",
    "    house_type = df_street_url_3[\"type\"][1],\n",
    "    url_addr = target[y].a.get(\"href\")\n",
    "\n",
    "    # Add and Clean: Scraped data to dict\n",
    "    dict_url_addr = {\n",
    "        \"zip\": \"\".join(str(zip_cd)).replace(\"(\",\"\").replace(\",\",\"\").replace(\")\",\"\"),\n",
    "        \"street\": \"\".join(str(street)).replace(\"(\",\"\").replace(\",\",\"\").replace(\")\",\"\").replace(\"'\",\"\"),\n",
    "        \"units\": \"\".join(str(units)).replace(\"(\",\"\").replace(\",\",\"\").replace(\")\",\"\"),\n",
    "        \"type\": \"\".join(str(house_type)).replace(\"(\",\"\").replace(\",\",\"\").replace(\")\",\"\").replace(\"'\",\"\"),\n",
    "        \"url_addr\": \"\".join(str(url_addr)).replace(\"(\",\"\").replace(\",\",\"\").replace(\")\",\"\")\n",
    "    }\n",
    "\n",
    "#     df_street_url_2 = pd.read_csv(\"Address URL List - FINAL.csv\")\n",
    "\n",
    "#     # Append: Dataframe to Database\n",
    "#     df_street_url_2 = df_street_url_2.append(dict_url_addr, ignore_index=True)\n",
    "\n",
    "#     # Save: Info to Dataframe\n",
    "#     df_street_url_2.to_csv(\"Address URL List - FINAL.csv\", index=False, columns=[\"zip\",\n",
    "#                                                                                  \"street\",\n",
    "#                                                                                  \"units\",\n",
    "#                                                                                  \"type\",\n",
    "#                                                                                  \"url_addr\"\n",
    "#                                                                                 ])\n",
    "    \n",
    "len(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is: LIVE\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in range(len(street_url_list)):\n",
    "    scrape_addr_urls(street_url_list[i], i)\n",
    "    counter = counter + 1\n",
    "    print(\"Scrape no: \" + str(counter) + \" completed. | URL processed: \" + str(street_url_list[i]) + \" | \" + str(len(street_url_list) - i) + \" remaining.\")\n",
    "\n",
    "print(\"Scrape Batch Completed\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
